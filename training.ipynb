{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"9ae25b5c29234ddbb3a54b71b8a9b764","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["model with 99% accuracy and 44kb size,  256 filters and alpha=.25 -----> 1671405237"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"bd387b88-8724-4f12-befe-5e72856b868e","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["model with 99% accuracy and 15kb size,  128 filters and alpha=.25 -----> 1671409506  "]},{"cell_type":"markdown","metadata":{"cell_id":"7d929197-149e-4318-a390-8fcc897460b8","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["model with 98% accuracy and 44kb size,  128 filters and alpha=.5 -----> 1671410314"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"41b96d0ec5e94b89b441eb5fc8310f42","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1671409460963,"source_hash":"febe4015","tags":[]},"outputs":[],"source":["import tensorflow as tf\n","import os\n","import numpy as np\n","import random\n"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"ca96bd56505d4f709336797b3e9c9619","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":404,"execution_start":1671409462872,"source_hash":"17d168e2","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-19 00:24:22.849346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2022-12-19 00:24:22.849382: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n","2022-12-19 00:24:22.849414: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-1971cb16-e709-4b3f-8d6d-f6f260f3f6b7): /proc/driver/nvidia/version does not exist\n","2022-12-19 00:24:22.849778: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["go_stop_train_ds = tf.data.Dataset.list_files(['msc-train/go*', 'msc-train/stop*'])\n","go_stop_val_ds = tf.data.Dataset.list_files(['msc-val/go*', 'msc-val/stop*'])\n","go_stop_test_ds = tf.data.Dataset.list_files(['msc-test/go*', 'msc-test/stop*'])"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"d777ea8f8dee459e8ae2ece012f8eb04","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1671409464219,"source_hash":"eadd08a7","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: 1600\n","Validation set size: 200\n","Test set size: 200\n"]}],"source":["num_train_files = len(go_stop_train_ds)\n","num_val_files = len(go_stop_val_ds)\n","num_test_files = len(go_stop_test_ds)\n","\n","print('Training set size:', num_train_files)\n","print('Validation set size:', num_val_files)\n","print('Test set size:', num_test_files)"]},{"cell_type":"code","execution_count":45,"metadata":{"cell_id":"8455d9894da94760a12b0a0f9ab0cdf5","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1671410269752,"source_hash":"41b36014","tags":[]},"outputs":[],"source":["TRAINING_ARGS = {\n","    'batch_size': 20,\n","    'initial_learning_rate': 0.01,\n","    'end_learning_rate': 1.e-5,\n","    'epochs': 10\n","}\n","final_sparsity = 0.70\n","alpha =0.5"]},{"cell_type":"code","execution_count":46,"metadata":{"cell_id":"8d38a16c882d455a861cdb4807f2d1d6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":37094,"execution_start":1671410270894,"source_hash":"c3764b59","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n","2022-12-19 00:37:51.072397: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","2022-12-19 00:37:51.074116: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","2022-12-19 00:37:51.074312: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n","2022-12-19 00:37:51.715594: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","2022-12-19 00:37:51.717556: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","2022-12-19 00:37:51.717764: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n","2022-12-19 00:37:52.087664: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","2022-12-19 00:37:52.089337: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","2022-12-19 00:37:52.089534: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n","2022-12-19 00:37:52.459176: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","2022-12-19 00:37:52.462879: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","2022-12-19 00:37:52.463094: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","\n","Batch Shape: (20, 32, 32, 1)\n","Data Shape: (32, 32, 1)\n","Labels: tf.Tensor([1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0], shape=(20,), dtype=int64)\n","2022-12-19 00:37:52.963213: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","Epoch 1/10\n","80/80 [==============================] - 7s 87ms/step - loss: 0.4668 - sparse_categorical_accuracy: 0.7794 - val_loss: 0.5387 - val_sparse_categorical_accuracy: 0.7900\n","Epoch 2/10\n","80/80 [==============================] - 3s 36ms/step - loss: 0.2503 - sparse_categorical_accuracy: 0.9050 - val_loss: 1.3702 - val_sparse_categorical_accuracy: 0.5100\n","Epoch 3/10\n","80/80 [==============================] - 3s 37ms/step - loss: 0.2001 - sparse_categorical_accuracy: 0.9225 - val_loss: 2.2539 - val_sparse_categorical_accuracy: 0.5200\n","Epoch 4/10\n","80/80 [==============================] - 3s 35ms/step - loss: 0.1672 - sparse_categorical_accuracy: 0.9350 - val_loss: 0.1842 - val_sparse_categorical_accuracy: 0.9450\n","Epoch 5/10\n","80/80 [==============================] - 3s 36ms/step - loss: 0.1312 - sparse_categorical_accuracy: 0.9513 - val_loss: 0.2080 - val_sparse_categorical_accuracy: 0.9350\n","Epoch 6/10\n","80/80 [==============================] - 3s 37ms/step - loss: 0.1132 - sparse_categorical_accuracy: 0.9581 - val_loss: 0.1783 - val_sparse_categorical_accuracy: 0.9450\n","Epoch 7/10\n","80/80 [==============================] - 3s 37ms/step - loss: 0.0929 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.1777 - val_sparse_categorical_accuracy: 0.9550\n","Epoch 8/10\n","80/80 [==============================] - 3s 36ms/step - loss: 0.0782 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.1804 - val_sparse_categorical_accuracy: 0.9500\n","Epoch 9/10\n","80/80 [==============================] - 3s 36ms/step - loss: 0.0664 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.1716 - val_sparse_categorical_accuracy: 0.9500\n","Epoch 10/10\n","80/80 [==============================] - 3s 36ms/step - loss: 0.0595 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.1548 - val_sparse_categorical_accuracy: 0.9550\n","10/10 [==============================] - 1s 45ms/step - loss: 0.0524 - sparse_categorical_accuracy: 0.9800\n","Training Loss: 0.0595\n","Training Accuracy: 97.75%\n","\n","Validation Loss: 0.1548\n","Validation Accuracy: 95.50%\n","\n","Test Loss: 0.0524\n","Test Accuracy: 98.00%\n","{'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'frame_step_in_s': 0.012, 'num_mel_bin': 10, 'lower_frequency': 20, 'upper_frequency': 8000, 'num_coefficients': 40, 'Training Accuracy': 97.75000214576721, 'Validation Accuracy': 95.49999833106995, 'Test Accuracy': 98.00000190734863}\n"]}],"source":["from preprocessing import LABELSB\n","from functools import partial\n","from collections import namedtuple\n","from itertools import product\n","import tensorflow_model_optimization as tfmot\n","\n","parameters = dict(\n","    \n","    downsampling_rate = [16000],#[16000,32000], \n","    frame_length_in_s = [0.016],     \n","    frame_step_in_s = [0.012],  \n","    num_mel_bin =[10],#[10, 40],\n","    lower_frequency =[20],#[20, 80],\n","    upper_frequency = [8000],#[2000, 8000],\n","    num_coefficients=[40],#[10,40]\n","    \n",")\n","\n","class RunBuilder():\n","  @staticmethod\n","  def get_runs(params):\n","    Run = namedtuple('Run', params.keys())\n","    runs=[]\n","    for v in product(*params.values()):\n","       runs.append(Run(*v))\n","    return runs\n","\n","def preprocess(filename):\n","        signal, downsmpling_rate,label = get_frozen_spectrogram(filename)\n","        signal.set_shape(SHAPE)\n","        signal = tf.expand_dims(signal, -1)\n","        signal = tf.image.resize(signal, [32, 32])\n","        label_id = tf.argmax(label == LABELSB)\n","\n","        return signal, label_id\n","\n","\n","from preprocessing import get_spectrogram,get_log_mel_spectrogram,LABELSB\n","tmplist=[]\n","for runs in RunBuilder.get_runs(parameters):\n","    PREPROCESSING_ARGS = {\n","    'downsampling_rate': runs.downsampling_rate,\n","    'frame_length_in_s': runs.frame_length_in_s,\n","    'frame_step_in_s': runs.frame_step_in_s,\n","    'num_mel_bin':runs.num_mel_bin,\n","    'lower_frequency':runs.lower_frequency,\n","    'upper_frequency':runs.upper_frequency,\n","    'num_coefficients':runs.num_coefficients\n","    \n","    }\n","\n","    # Use Fallowing code to train on spectrogram\n","    #get_frozen_spectrogram = partial(get_spectrogram, **PREPROCESSING_ARGS)\n","    #final_ds = go_stop_train_ds.map(get_frozen_spectrogram)    \n","    #for spectrogram, downsampling_rate, label in final_ds.take(1):\n","     #  SHAPE = spectrogram.shape\n","\n","    \n","    # Use Following code to train on mfcc    \n","    get_frozen_spectrogram = partial(get_log_mel_spectrogram, **PREPROCESSING_ARGS)\n","    final_ds = go_stop_train_ds.map(get_frozen_spectrogram)\n","    for spectrogram, downsampling_rate, label in final_ds.take(1):\n","        SHAPE = spectrogram.shape\n","    batch_size = TRAINING_ARGS['batch_size']\n","    epochs = TRAINING_ARGS['epochs']\n","\n","    train_ds = go_stop_train_ds.map(preprocess).batch(batch_size).cache()\n","    val_ds = go_stop_val_ds.map(preprocess).batch(batch_size)\n","    test_ds = go_stop_test_ds.map(preprocess).batch(batch_size)\n","   \n","\n","    for example_batch, example_labels in train_ds.take(1):\n","        print()\n","        print('Batch Shape:', example_batch.shape)\n","        print('Data Shape:', example_batch.shape[1:])\n","        print('Labels:', example_labels)\n","   \n","    \n","    model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n","    tf.keras.layers.Conv2D(filters=int(128*alpha), kernel_size=[3, 3], strides=[2, 2],\n","        use_bias=False, padding='valid'),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.ReLU(),\n","    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n","        use_bias=False, padding='same'),\n","    tf.keras.layers.Conv2D(filters=int(128*alpha), kernel_size=[1, 1], strides=[1, 1],   \n","       use_bias=False),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.ReLU(),\n","    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n","        use_bias=False, padding='same'),\n","    tf.keras.layers.Conv2D(filters=int(128*alpha), kernel_size=[1, 1], strides=[1, 1],   \n","       use_bias=False),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.ReLU(),\n","    tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.layers.Dense(units=len(LABELSB)),\n","    tf.keras.layers.Softmax()\n","])\n","\n","    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","\n","    begin_step = int(len(go_stop_train_ds) * epochs * 0.2)\n","    end_step = int(len(go_stop_train_ds) * epochs)\n","\n","    pruning_params = {\n","    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n","        initial_sparsity=0.20,\n","        final_sparsity=final_sparsity,\n","        begin_step=begin_step,\n","        end_step=end_step\n","    )\n","}\n","\n","    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n","\n","    # model.summary()\n","    \n","\n","    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n","    initial_learning_rate = TRAINING_ARGS['initial_learning_rate']\n","    end_learning_rate = TRAINING_ARGS['end_learning_rate']\n","\n","    linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n","        initial_learning_rate=initial_learning_rate,\n","        end_learning_rate=end_learning_rate,\n","        decay_steps=len(train_ds) * epochs,\n","    )\n","    optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n","    metrics = [tf.metrics.SparseCategoricalAccuracy()]\n","    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","\n","    history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n","    test_loss, test_accuracy = model.evaluate(test_ds)\n","    training_loss = history.history['loss'][-1]\n","    training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n","    val_loss = history.history['val_loss'][-1]\n","    val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n","\n","    print(f'Training Loss: {training_loss:.4f}')\n","    print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n","    print()\n","    print(f'Validation Loss: {val_loss:.4f}')\n","    print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n","    print()\n","    print(f'Test Loss: {test_loss:.4f}')\n","    print(f'Test Accuracy: {test_accuracy*100.:.2f}%')\n","    record = {\n","    'downsampling_rate': runs.downsampling_rate,\n","    'frame_length_in_s': runs.frame_length_in_s,\n","    'frame_step_in_s': runs.frame_step_in_s,\n","    'num_mel_bin':runs.num_mel_bin,\n","    'lower_frequency':runs.lower_frequency,\n","    'upper_frequency':runs.upper_frequency,\n","    'num_coefficients':runs.num_coefficients,\n","    'Training Accuracy': training_accuracy*100,\n","    'Validation Accuracy': val_accuracy*100,\n","    'Test Accuracy':test_accuracy*100\n","    }\n","    tmplist.append(record)\n","    print(record)"]},{"cell_type":"code","execution_count":47,"metadata":{"cell_id":"b7f1a6c8c85342b6a475ebfb27cbb7a6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1122,"execution_start":1671410314908,"source_hash":"2c1aadfe","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n","INFO:tensorflow:Assets written to: ./saved_models/1671410314/assets\n","INFO:tensorflow:Assets written to: ./saved_models/1671410314/assets\n"]}],"source":["from time import time\n","\n","timestamp = int(time())\n","\n","saved_model_dir = f'./saved_models/{timestamp}'\n","if not os.path.exists(saved_model_dir):\n","    os.makedirs(saved_model_dir)\n","model.save(saved_model_dir)"]},{"cell_type":"code","execution_count":48,"metadata":{"cell_id":"a0764f6a858c4223bc29a59b159fb7f7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1671410325971,"source_hash":"2528c562","tags":[]},"outputs":[],"source":["import pandas as pd\n","\n","output_dict = {\n","    'timestamp': timestamp,\n","    **PREPROCESSING_ARGS,\n","    **TRAINING_ARGS,\n","    'test_accuracy': test_accuracy\n","}\n","\n","df = pd.DataFrame([output_dict])\n","\n","output_path='./spectrogram_dscnn_results.csv'\n","df.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)\n"]},{"cell_type":"code","execution_count":49,"metadata":{"cell_id":"e0d533f3e4c1405388be06b1b2f63729","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":724,"execution_start":1671410333716,"source_hash":"656d99aa","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-19 00:38:54.152188: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n","2022-12-19 00:38:54.152243: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n","2022-12-19 00:38:54.152385: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/1671410314\n","2022-12-19 00:38:54.155238: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n","2022-12-19 00:38:54.155285: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/1671410314\n","2022-12-19 00:38:54.164475: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n","2022-12-19 00:38:54.233642: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/1671410314\n","2022-12-19 00:38:54.248554: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 96169 microseconds.\n"]}],"source":["converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{1671409506}')\n","tflite_model = converter.convert()"]},{"cell_type":"code","execution_count":50,"metadata":{"cell_id":"282b5fef3701422fb50cfa8d98049b66","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1671410341852,"source_hash":"5eb9fb99","tags":[]},"outputs":[],"source":["tflite_models_dir = './tflite_models'\n","if not os.path.exists(tflite_models_dir):\n","    os.makedirs(tflite_models_dir)"]},{"cell_type":"code","execution_count":51,"metadata":{"cell_id":"ec5670aacdf941d8aa3b6b26406f8bbc","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1671410348370,"source_hash":"a3eb5317","tags":[]},"outputs":[{"data":{"text/plain":["'./tflite_models/1671410314.tflite'"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["tflite_model_name1 = os.path.join(tflite_models_dir, f'{1671409506}.tflite')\n","tflite_model_name1"]},{"cell_type":"code","execution_count":52,"metadata":{"cell_id":"343416a2461e4c34b08e20401b19d84d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1671410352546,"source_hash":"d46c1540","tags":[]},"outputs":[],"source":["with open(tflite_model_name1, 'wb') as fp:\n","    fp.write(tflite_model)"]},{"cell_type":"code","execution_count":53,"metadata":{"cell_id":"d07a18e20d0540c38c41924e3ef4d9d9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1671410355640,"source_hash":"292e9271","tags":[]},"outputs":[],"source":["import zipfile\n","\n","with zipfile.ZipFile(f'{tflite_model_name1}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(tflite_model_name1)"]},{"cell_type":"code","execution_count":54,"metadata":{"cell_id":"e231ca392fef4ea289925a1f426ceeba","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1671410361404,"source_hash":"cb285807","tags":[]},"outputs":[],"source":["import zipfile\n","\n","not_pruned_tflite = os.path.join(tflite_models_dir, '1671409506.tflite')\n","\n","with zipfile.ZipFile(f'{not_pruned_tflite}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(not_pruned_tflite)  "]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"587361bc86ef4ae9b5970aedc5c4cb5e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1671410373005,"source_hash":"3fc05054","tags":[]},"outputs":[],"source":["not_pruned_tflite_size = os.path.getsize(not_pruned_tflite) / 1024.0\n","tflite_size = os.path.getsize(tflite_model_name1) / 1024.0\n","not_pruned_zipped_size = os.path.getsize(f'{not_pruned_tflite}.zip') / 1024.0\n","zipped_size = os.path.getsize(f'{tflite_model_name1}.zip') / 1024.0\n","\n","print(f'Original tflite size (not pruned model): {tflite_size:.3f} KB')\n","print(f'Original tflite size (pruned model): {tflite_size:.3f} KB')\n","print(f'Zipped tflite size (not pruned model): {not_pruned_zipped_size:.3f} KB')\n","print(f'Zipped tflite size (pruned model): {zipped_size:.3f} KB')"]},{"cell_type":"code","execution_count":56,"metadata":{"cell_id":"c64263605d0d463baebfe8f20dc24d42","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1671410383817,"source_hash":"135f5f60","tags":[]},"outputs":[{"data":{"text/plain":["'./tflite_models/1671410314.tflite'"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["MODEL_NAME = tflite_model_name1\n","MODEL_NAME"]},{"cell_type":"code","execution_count":57,"metadata":{"cell_id":"7b5ea19a6651435b96b94e6b4496d454","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10,"execution_start":1671410387468,"source_hash":"c4af3ae2","tags":[]},"outputs":[],"source":["downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\n","sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n","frame_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_length_in_s'])\n","frame_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\n","spectrogram_width = (16000 - frame_length) // frame_step + 1\n","num_spectrogram_bins = frame_length // 2 + 1\n","\n","linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n","    PREPROCESSING_ARGS['num_mel_bin'],\n","    num_spectrogram_bins,\n","    downsampling_rate,\n","    PREPROCESSING_ARGS['lower_frequency'],\n","    PREPROCESSING_ARGS['upper_frequency']\n",")"]},{"cell_type":"code","execution_count":84,"metadata":{"cell_id":"fb41ee3db3b7414c85f51554ae46effe","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":29,"execution_start":1671410805069,"source_hash":"1ce08e5c","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of inputs: 1\n","Number of outputs: 1\n","Input name: serving_default_input_2:0\n","Input shape: [ 1 32 32  1]\n","Input index: 0\n","Output name: StatefulPartitionedCall:0\n","Output shape: [1 2]\n"]}],"source":["interpreter1 = tf.lite.Interpreter(model_path='tflite_models/1671409506.tflite')\n","interpreter1.allocate_tensors()\n","\n","input_details = interpreter1.get_input_details()\n","output_details = interpreter1.get_output_details()\n","\n","print(\"Number of inputs:\", len(input_details))\n","print(\"Number of outputs:\", len(output_details))\n","print(\"Input name:\", input_details[0]['name'])\n","print(\"Input shape:\", input_details[0]['shape'])\n","print(\"Input index:\", input_details[0]['index'])\n","print(\"Output name:\", output_details[0]['name'])\n","print(\"Output shape:\", output_details[0]['shape'])"]},{"cell_type":"code","execution_count":85,"metadata":{"cell_id":"06c48242806640eb8584eeb2714aa9ac","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1671410808063,"source_hash":"50f406e6","tags":[]},"outputs":[],"source":["downsampling_rate= 16000\n","frame_length_in_s= 0.016\n","frame_step_in_s= 0.012\n","num_mel_bins= 10\n","lower_frequency= 20\n","upper_frequency= 8000\n","num_coefficients=40"]},{"cell_type":"code","execution_count":86,"metadata":{"cell_id":"261ef7d7de5e450eb9b24324dca7be3d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9,"execution_start":1671410809178,"source_hash":"d9e8bf8d","tags":[]},"outputs":[],"source":["sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n","frame_length = int(downsampling_rate * frame_length_in_s)\n","frame_step = int(downsampling_rate * frame_step_in_s)\n","spectrogram_width = (16000 - frame_length) // frame_step + 1\n","num_spectrogram_bins = frame_length // 2 + 1\n","\n","linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n","    num_mel_bins,\n","    num_spectrogram_bins,\n","    downsampling_rate,\n","    lower_frequency,\n","    upper_frequency\n",")"]},{"cell_type":"code","execution_count":87,"metadata":{"cell_id":"4e94e4b479144393b8792c7b639bb334","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":12,"execution_start":1671410811108,"source_hash":"4a62f6c1","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'name': 'serving_default_input_2:0', 'index': 0, 'shape': array([ 1, 32, 32,  1], dtype=int32), 'shape_signature': array([-1, 32, 32,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"]}],"source":["print(interpreter1.get_input_details())"]},{"cell_type":"code","execution_count":88,"metadata":{"cell_id":"dfc9f549031a43e7b7754ac9b76a620d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2517,"execution_start":1671410813532,"source_hash":"11a47300","tags":[]},"outputs":[],"source":["\"\"\"\n","Inference \n","\"\"\"\n","from glob import glob\n","from time import time\n","import tensorflow_io as tfio\n","\n","\n","#filenames = glob('msc-test/*')\n","filenames = glob('msc-test/go*') + glob('msc-test/stop*')\n","\n","\n","avg_preprocessing_latency = 0.0\n","avg_model_latency = 0.0\n","avg_total_latency = 0.0\n","accuracy = 0.0\n","\n","for filename in filenames:\n","    audio_binary = tf.io.read_file(filename)\n","    path_parts = tf.strings.split(filename, '/')\n","    path_end = path_parts[-1]\n","    file_parts = tf.strings.split(path_end, '_')\n","    true_label = file_parts[0]\n","    true_label = true_label.numpy().decode()\n","    \n","    start_preprocess = time()\n","    audio, sampling_rate = tf.audio.decode_wav(audio_binary) \n","    audio = tf.squeeze(audio)\n","    zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n","    audio_padded = tf.concat([audio, zero_padding], axis=0)\n","\n","    if downsampling_rate != sampling_rate:\n","        audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n","\n","    stft = tf.signal.stft(\n","        audio_padded, \n","        frame_length=frame_length,\n","        frame_step=frame_step,\n","        fft_length=frame_length\n","    )\n","    spectrogram = tf.abs(stft)\n","\n","    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n","    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n","    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)[..., :num_coefficients]\n","\n","    \n","    SHAPE=mfccs.shape\n","    mfccs.set_shape(SHAPE)\n","    mfccs = tf.expand_dims(mfccs, 0)\n","    mfccs = tf.expand_dims(mfccs, -1)\n","    \n","    mfccs = tf.image.resize(mfccs, [32, 32])\n","   \n","    label_id = tf.argmax(label == LABELSB)\n","    \n","    end_preprocess = time()\n","\n","    interpreter1.set_tensor(input_details[0]['index'], mfccs)\n","\n","    interpreter1.invoke()\n","    output = interpreter1.get_tensor(output_details[0]['index'])\n","\n","    end_inference = time()\n","\n","    top_index = np.argmax(output[0])\n","    predicted_label = LABELSB[top_index]\n","\n","    accuracy += true_label == predicted_label\n","    avg_preprocessing_latency += end_preprocess - start_preprocess\n","    avg_model_latency += end_inference - end_preprocess\n","    avg_total_latency += end_inference - start_preprocess"]},{"cell_type":"code","execution_count":89,"metadata":{"cell_id":"cbbcd6a883644b87a27f10af78b2db35","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1671410818945,"source_hash":"17ca417e","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["196.0\n"]}],"source":["print(accuracy)\n","accuracy /= len(filenames)\n","avg_preprocessing_latency /= len(filenames)\n","avg_model_latency /= len(filenames)\n","avg_total_latency /= len(filenames)"]},{"cell_type":"code","execution_count":90,"metadata":{"cell_id":"6ad3a4eafa5a41abb25c0de30aab1b35","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1671410821752,"source_hash":"aaec1945","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 98.000%\n","Preprocessing Latency: 6.3ms\n","Model Latency: 0.2ms\n","Total Latency: 6.5ms\n"]}],"source":["print(f'Accuracy: {100 * accuracy:.3f}%')\n","# print(f'Model size: {model_size / 2 ** 10:.1f}KB')\n","print(f'Preprocessing Latency: {1000 * avg_preprocessing_latency:.1f}ms')\n","print(f'Model Latency: {1000 * avg_model_latency:.1f}ms')\n","print(f'Total Latency: {1000 * avg_total_latency:.1f}ms')"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1510e8601b0e4ab0955ee2652bf5d94a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1671410827778,"source_hash":"ea1786fb","tags":[]},"outputs":[],"source":["import os\n","model_size = os.path.getsize(f'tflite_models/1671409506.tflite')/1024\n","print(model_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7fe7ecce942545769552cec608c874e3","deepnote_cell_type":"code","tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","tags":[]},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1971cb16-e709-4b3f-8d6d-f6f260f3f6b7' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"7651d63f1e1a4fdab91f2805d43bb269","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]"},"orig_nbformat":2,"vscode":{"interpreter":{"hash":"97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"}}},"nbformat":4,"nbformat_minor":0}
